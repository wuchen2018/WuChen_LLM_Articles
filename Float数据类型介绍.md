
- [什么是模型量化](#什么是模型量化)
    - [定义](#定义)
    - [好处](#好处)
- [浮点数](#浮点数)
    - [FP32：单精度浮点数](#FP32：单精度浮点数)
    - [关于符号位、指数位、尾数位的计算规则](#关于符号位、指数位、尾数位的计算规则)
    - [FP16：半精度浮点数](#FP16：半精度浮点数)
    - [FP64：双精度浮点数](#FP64：双精度浮点数)
- [半精度训练产生的两个问题](#半精度训练产生的两个问题)

# 什么是模型量化
## 定义
模型量化是一种深度学习优化技术，它通过减少模型中的权重和激活函数的数值精度来减小模型大小和加速模型的推理速度。

模型量化就像是把一个语言从使用丰富词汇表达，简化为只用基本词汇交流。在这个比喻中，语言是深度学习模型，词汇则对应模型中的数字。就像用更简单的词汇可以加快交流速度，减少说话人的记忆负担，模型量化也减少了模型的大小，加快了运算速度，并降低了硬件的计算负荷。

例如，如果一个模型最初用32位来存储每个数字（这相当于一个非常丰富的词汇表），量化后可能只用8位（更简单的词汇表）。但减少使用的词汇可能会使得表达不够精确。同样，在模型中，通过减少数字的“精度”，我们可能会失去一些信息，这可能影响模型的性能。

整个量化的过程是一种折衷，通过减少精度来交换更高的效率。在很多情况下，这样做是可以接受的，尤其是当部署模型到资源受限的设备（如手机或嵌入式设备）上时。

## 好处
模型量化的主要好处包括：

降低存储需求： 使用更少的位可以显著减少模型存储在磁盘上的空间和在内存中的占用，因此减少硬件资源消耗。
提高推理速度： 降低计算复杂度可以加快模型在单片机、移动设备和边缘设备上的推理速度，这是因为整数计算通常比浮点计算要快，并可以更有效地使用硬件加速。
减少能耗： 由于需要处理的数据位数减少，因此减少了运算器件（如CPU和GPU）的能量消耗，这对于电池供电的设备尤其重要。


# 浮点数
在研究模型量化之前，需要先了解常见的数据类型和对应的数据精度的概念。数据精度是信息表示的精细程度，这在计算机中是由数据类型和其位数决定的。

## FP32：单精度浮点数
FP32（float32），或称为单精度浮点数，使用32位表示一个浮点数。这些位被分为三个部分：1位符号位，8位指数位和23位尾数位。尾数提供了数字的精度，而指数决定了数字能表达的范围大小。浮点数表示的本质是，将一个数字分解为基数（尾数）乘以一个2的指数。尾数提供了数字的精确度，而指数则定义了数值的大小。

单精度浮点数能表示的精度大约为1.19e-07，即2**(-23)的结果。换句话说，float32的数据类型的有效小数位是小数点后7位。超过7位后，会被认为二者相等。下面的代码演示了这一原理。
```python
import numpy as np

# 将数字转换为float32类型
f1 = np.float32(12.12345678)
f2 = np.float32(12.12345679)

# 打印转换为float32之后的数值
print(f1)  # 输出: 12.123456
print(f2)  # 输出: 12.123456

# 检查它们是否相等
print(f1 == f2)  # Depending on the precision, this might be True or False
```
单精度数能表示的数值范围相当宽广，float32可以表示的数值范围约为2^-126（大约1.18e-38）到2^127（大约3.4e+38），负数范围与此对称。下面的代码可以看出float32数据类型的表示范围：
```python
import numpy as np

# 获取float32数据类型的机器极限
float32_info = np.finfo(np.float32)

print("float32的最大正数：", float32_info.max)
print("float32的最小正规约化数：", float32_info.min)
print("float32的最小正非规约化数（下溢数）：", float32_info.tiny)
print("float32的精度或最小正数的解析度：", float32_info.eps)
```
运行结果：

float32的最大正数： 3.4028235e+38
float32的最小正规约化数： -3.4028235e+38
float32的最小非规格化数（下溢数）： 1.1754944e-38
float32的精度或最小正数的解析度： 1.1920929e-07
正规约化数的概念比较复杂，暂时可以跳过。

注意：表示范围和表示精度是不同的概念，有人可能会问，为什么一方面，float32表示的有效的小数位只有7位小时，但是可以表示e-38的数？

以下是GPT4的回答：

理解float32（单精度浮点数）可以表示大约1.18e-38这样的数，而其有效小数位数约为6到7位，需要了解浮点数的表示方式。浮点数由符号位、指数位和尾数位三部分组成。

在32位的浮点数（即float32）中，这三部分的位数分配如下：

1位用于符号（表示正数或负数）
8位用于指数（使用偏移或“移码”表示法）
23位用于尾数（又称为小数部分）
尾数位数决定了精度，也就是数字的有效位数。在标准的浮点表示（IEEE 754）中，浮点数的尾数部分隐含了一个始终为1的有效数的最高位。这意味着对于float32：

尾数实际上能表示的信息是24位。
这种表示方式称为“隐含的前导1”。
24位二进制数字大致等同于7位十进制数字。
浮点数的指数部分定义了数值的范围。它告诉我们数字的小数点应该放在哪里。当指数非常小（接近其最小可能值）时，即使尾数只能精确到大约6到7个小数位，也能表示非常小的数，因为浮点数在此情况下表示非常接近于零的值。

举例来说，当尾数是逼近1的数（例如，1.0000000），并且指数是非常小的负数时，结果就是一个非常小的正数，这是因为我们将一个接近1的数乘以2的非常小的负数次幂。由于指数部分的范围，这意味着float32可以表示非常小的数，比如1.18e-38，即使其尾数的绝对精度仅限于6到7个有效数字。

综上所述，虽然float32的尾数只能表示大约6到7位有效数字的精度，但由于指数位的存在和其编码方式的灵活性，它可以表示的数值范围非常宽，从非常小的数（例如1.18e-38）到非常大的数（例如3.4e+38）都是可能的。

如果GPT的回答没有让你弄懂，可以看下面——关于符号位、指数位、尾数位的计算规则。

## 关于符号位、指数位、尾数位的计算规则



我使用一个具体的数值来解释IEEE 754的浮点数表示法。以值 6.75 为例，来执行这个转换过程。

在二进制中，6.75 可以表示为 110.11，因为：

4 (2²) 的二进制表示是 100
2 (2¹) 的二进制表示是 010
0.5 (2⁻¹) 的二进制表示是 .1
0.25 (2⁻²) 的二进制表示是 .01
相加得到二进制的 110.11。现在，按照IEEE 754的32位浮点数格式，我们需要把这个数表示为1.xxxxx的形式，并计算指数。我们把二进制点向左移动2个位置（这里指数是2），那么我们可以把 6.75 写成 1.1011 * 2² 的形式。

现在，参照IEEE 754的32位单精度浮点标准：

符号位：因为 6.75 是正数，所以符号位是 0。
指数：指数为2，在其基础上加上127（因为IEEE 754的偏移量是127），得到129。129的二进制是 10000001。
尾数：尾数是 1.1011 中除了隐含的那个1之外的部分，即小数 1011。由于尾数部分在IEEE 754格式中达23位，我们需要在这个小数后面添加0，直至长度为23位。所以 1011 后面跟了19个0：10110000000000000000000。
组合这些部分，我们得到了 6.75 的32位IEEE 754浮点数表示：

0 10000001 10110000000000000000000
每一部分分开解释是：

符号位：0 (因为它是正数)
指数：10000001 (等于2加上偏移量127，在二进制中等于129)
尾数：10110000000000000000000 (表示 .1011 并在后面填充0直至总长为23位)
如果你将上述二进制串转换成32位IEEE 754格式的二进制串，它将会是：

01000000110110000000000000000000
用代码验证：
```python
import struct

# 给定一个float32数值
float_value = 6.75

# 使用 'struct.pack' 将float转为4字节的二进制数据，'f' 表示float32
packed = struct.pack('f', float_value)

# 使用 'struct.unpack' 将4字节的二进制数据解析为对应的二进制整数，'I' 表示unsigned int
binary_int = struct.unpack('I', packed)[0]

# 将整数转为32位的二进制字符串
binary_string = format(binary_int, '032b')

print(binary_string)
```
## FP16：半精度浮点数
FP16，也称为半精度浮点数，它使用16位二进制数来表示浮点数，这16位中，1位用于符号位，5位用于指数位，10位用于尾数位。

## FP64：双精度浮点数
FP64，也被称为双精度浮点数，使用64位来表示，其中1位是符号位，11位是指数位，52位是尾数位。双精度浮点数的最小正非零数大约是2.220e-16，即2**(-52)的结果。双精度浮点数因为更长的尾数，提供更高的精度，达到大约15到16位十进制有效数字，且表示范围更广，大约为4.9e-324到1.7e308。



# 半精度训练产生的两个问题
如果直接使用半精度进行计算会导致的两个问题的处理：舍入误差(Rounding Error)和溢出错误(Grad Overflow上溢 / Underflow下溢)。

## 舍入误差
舍入误差是数字计算中的常见现象，特别是在处理浮点数时。它发生在数字由于存储或计算的限制而不能精确表示时。由于计算机使用固定数量的位来存储浮点数，当一个数不可能准确地适配这个限定的位数时，就需要通过舍入操作进行近似。

举几个导致舍入误差的原因：

有限尾数位数：
现实中许多数字是无限小数，例如1/3或π（pi），但在浮点数表示中只有有限的尾数位数（如float32中的23位尾数位）。因此，当这些数字被转换为浮点数时，只能保留一定数量的有效数字，尾数位数之外的部分被舍去或四舍五入，造成舍入误差。
非二进制基数的数字：
二进制系统不能精确表示所有十进制小数。例如，十进制中的0.1在二进制中是一个无限循环小数，类似于十进制中的1/3。当0.1这样的数字在二进制中被四舍五入到最接近的可表示的浮点数时，就会产生舍入误差。
运算中的累积误差：
进行多步骤的数学运算时（特别是加减运算），每一步都可能引入小量的舍入误差。如果这些舍入误差在计算过程中不断累积，最后的结果可能会明显偏离真实值。
大数与小数相加：
当大数和小数相加时，如果小数远小于大数，那么小数的贡献可能在舍入过程中完全消失，这是因为大数占用的尾数位数没有留下足够的空间去表示小数部分。
舍入误差是计算机所固有的限制，对于要求高精确度的计算任务来说是一个挑战。例如，在科学计算、金融计算等领域中，舍入误差可能会导致明显的问题。处理这类问题通常需要精心设计算法以最小化误差的影响，或使用更高精度的数据类型（如float64代替float32）以提供更多的尾数位数，以降低舍入误差的产生概率。在某些情况下，特别是涉及金融计算时，也会使用定点数代替浮点数以避免舍入误差。

以下是代码演示：
```python
import numpy as np

value = np.float32(1.0)
increment = np.float32(1e-15)  

print(value + increment)

value = np.float32(1.0)
increment = np.float32(1e-5)  

print(value + increment) 
```
输出：

1.0
1.00001
通俗来说，就是float32类型的数据最多能表示小数点后7位的数字，再往后的数据被抹掉了，被“四舍五入”了。

再举个例子：
```python
a = np.float64(1.2345678)
b = np.float64(1.2345678)
print(a*b)
a = np.float32(1.2345678)
b = np.float32(1.2345678)
print(a*b)
```
输出：

1.52415765279684
1.5241575
可以看到，两个float32的数字相乘，由于float32类型的数据最多能表示小数点后7位的数字，相乘后的结果需要只保留前七位。

## 溢出错误
浮点数运算时的溢出错误，就像是有一把尺子去度量非常非常大的数值，但这把尺子的刻度仅限一定范围。如果你尝试测量超过这个尺子长度的东西，它就“溢出”了，也就是没法准确显示结果。

在计算机里，浮点数通常用来表示实数，也就是小数点可以浮动的数。计算机用一个固定数量的位来存储这些数，就好比尺子有固定的长度和刻度。这些位按部分来划分——一部分用来存储主要的数字（称作尾数），另一部分存储这个数字可以放到哪个位置（称作指数），最后一部分表示这个数是正是负。

当你进行一个特别大的浮点数运算，比如说两个非常大的数相乘，结果可能超过计算机“尺子”的最大刻度。也就是说，这个结果大到连最大的位置（指数）都放不下了。当这发生时，计算机通常会把这个结果显示为“无穷大”（Infinity），这是计算机表达“超出我能度量的范围了”之意。

类似地，如果你让一个非常小的浮点数和一个特别大的数相除，结果可能比“尺子”的最小刻度还要小。但在实际应用中，浮点数的下溢错误通常通过转换成0来处理。

总之，浮点数溢出就是说，你试图在计算机中表示一个超出它所能处理的数值范围的数。在大多数情况下，它会给出一些特殊值（如Infinity）来告诉你：这个数太大或太小了，我处理不来。

## 舍入误差和溢出错误的区别
舍入误差涉及到对无法精确表示的数字进行近似，这种误差是数值相对大小造成的，而且是固有的、不可避免的。
溢出错误涉及到数值超出了数值类型的存在边界，导致数值变成负无穷、正无穷或者循环回到数据类型的起始。

## 深度学习中的舍入误差和溢出错误
在深度学习中，舍入误差和溢出错误都是可能发生的，在某些情况下它们会对模型的训练和推理产生影响。

舍入误差：

由于深度学习涉及到大量的数值计算，如权重更新、激活函数计算、损失函数计算等，因此舍入误差是这一领域的一个常见问题。舍入误差主要发生在：

有限精度：在实践中，深度学习模型中的权重和中间计算结果通常使用32位或16位的浮点数（float32或float16）表示。由于浮点数的表示精度有限，无法精确表示所有真实数值，导致运算过程中会有舍入。
参数更新：在训练过程中，参数通常通过梯度下降算法进行更新。梯度以及学习率可能是很小的数，当它们与当前权重相结合时，可能因为数值变化太小而在数值表示上丢失，导致更新无效。
量化：为了加速推理并减小模型大小，深度学习模型可能被量化到更低的位宽比如8位整数（int8）。在这种情况下，舍入误差可能会更加显著。
比如，两个不同的数据，计算的激活值是1.2345和1.2346，但是由于舍入误差，会认为是同一个数据。又比如参数减去更新值是0.54-0.46=0.08，但是实际上的网络只能精确到小数点后1位，变成了0.5-0.5=0。
至于说梯度0.00002被舍入成0，我认为更应该理解成下溢出（数据表示超出了最小的正整数）。

溢出错误：
溢出错误在深度学习中虽然较少见，但也可能发生在梯度爆炸过程中，即在训练过程中，如果梯度过大，容易导致溢出。这是梯度爆炸问题的一个例证，其中权重更新变得异常大，使模型参数迅速偏离合理范围。
除此之外，计算log和指数的时候也可能发生，（softmax/cross_entropy）
